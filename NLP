from transformers import BertTokenizer, TFBertForQuestionAnswering

# Load BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertForQuestionAnswering.from_pretrained('bert-base-uncased')

# Preprocess the input text (question and context)
inputs = tokenizer(question, context, return_tensors='tf')

# Get the model's output
outputs = model(**inputs)

# Extract the answer from the model's output
start_logits, end_logits = outputs.start_logits, outputs.end_logits
start_index = tf.argmax(start_logits, axis=1)
end_index = tf.argmax(end_logits, axis=1)

answer = context[start_index[0] : end_index[0] + 1]
print(answer)
